## üéØ Overview

This MVP delivers a backend system for meeting audio transcription, diarization, and summarization with accent-aware and noise-resilient processing. Users upload recorded meeting audio, which is processed to produce a clean transcript, speaker-separated segments, extracted key points, and an actionable summary. Export formats and APIs provide the processed data for frontend integration. The stack is designed to be production-grade but leaves advanced features (RAG, live streaming, domain fine-tuning, feedback loops) for stage two.

---

## üîß Technical Details

### 1. **Audio Ingestion & Storage**

- **Input format:** WAV/MP3/M4A. Max size ~1 GB per file for MVP.
- **Flow:**
    - User uploads via signed URL generated by E2E object storage API.
    - Metadata stored in DB: `file_id`, `user_id`, `upload_time`, `status`.

### 2. **Pre-processing**

- **RNNoise**: Reduce background noise.
- **VAD (Silero/py-webrtcvad)**: Trim silence, split long audio into ~30‚Äì60 s chunks.
- **Chunk metadata:** Stored to preserve original timestamps.

### 3. **Transcription**

- **Model:** Whisper Large or WhisperX (better timestamps and accent support).
- **Settings:** Beam size=5, temperature=0.0‚Äì0.2, English + Hindi code-switch mode.
- **Output:** JSON with `text`, `start`, `end`, `confidence`.

### 4. **Diarization**

- **Model:** Pyannote 3.1 pipeline (pretrained).
- **Functionality:**
    - Identify speakers (speaker1, speaker2, etc.).
    - Overlap detection flags.
    - Map transcription chunks to speaker segments.

### 5. **Language & Code-Switch Detection**

- **Library:** FastText or langdetect.
- Tag each segment as `en`, `hi`, or `mixed`.
- Used to preserve meaning in code-switched segments.

### 6. **Topic Segmentation**

- **Method:** Sentence embeddings (Instructor-XL or all-MiniLM) + clustering (Agglomerative).
- Used to split transcript into logical sections for summarization.

### 7. **Key Information Extraction**

- **Tools:** KeyBERT or YAKE for key phrases.
- **NER:** SpaCy (English/Hindi model) for company names, products, locations.
- Output: Structured JSON with `keywords`, `entities`.

### 8. **Summarization**

- **Method:** Two-pass:
    - Extractive: Select most important sentences (TextRank/BART extractive).
    - Abstractive polish: FLAN-T5 with temperature=0.2 for low hallucination.
- Summary length: ~5‚Äì10% of transcript size.
- Output includes: executive summary, bullet points, action items.

### 9. **Actionables & Pain Points**

- Pattern-based extraction:
    - Regex + shallow NLP patterns for ‚ÄúWe need‚Ä¶‚Äù, ‚ÄúProblem is‚Ä¶‚Äù, ‚ÄúCan you‚Ä¶‚Äù.
    - Map to timestamps and speakers.

### 10. **Analytics**

- Talk-time per speaker (sum of segment durations).
- Overlap count (interruptions).
- Keyword heatmap: frequency over timeline.

### 11. **Exports**

- JSON: Full structured data.
- Markdown: Human-readable transcript + summary.
- VTT/SRT: For video subtitles.

### 12. **API Layer**

- Endpoints:
    - `POST /upload` ‚Üí signed URL + file metadata
    - `POST /process` ‚Üí triggers pipeline
    - `GET /status/{file_id}`
    - `GET /results/{file_id}?format=json|md|srt`
- Auth: Token-based (basic for MVP).

### 13. **Quality Flags**

- Word Error Rate (WER) on each chunk.
- Diarization Error Rate (DER) rough estimate.
- Segment confidence scores.

### 14. **Testing**

- Unit tests: preprocessing, transcription, diarization.
- Integration: full pipeline run with sample audio.
- Smoke tests: upload, process, export.
- Regression: test with noisy Indian-English + code-switched samples.

---

## ‚ö° Performance Considerations

- **Batching:** Process audio in parallel chunks to reduce Whisper inference time.
- **GPU usage:** One GPU node for transcription & summarization.
- **Memory:** Each job <2 GB RAM if chunked.
- **Scaling:** Queue-based processing (e.g., Celery/RabbitMQ).
- **Latency goal:** ‚â§15 min for 1-hour audio.

---

## üß™ Testing Strategy

- **Unit Tests:**
    - RNNoise processing integrity.
    - VAD chunking accuracy.
    - Whisper output sanity check.
- **Integration Tests:**
    - End-to-end: noisy Indian-English sample ‚Üí transcript ‚Üí summary.
- **Regression Tests:**
    - Accent-heavy datasets.
    - Hindi-English mixed content.
- **Manual Review:**
    - Human-in-loop validation for first 5 recordings.

---

## üìã Dependencies & Requirements

- **Core Models:**
    - Whisper Large / WhisperX
    - Pyannote 3.1
    - qwen 2.5 14b 
    - KeyBERT or YAKE
    - SpaCy en_core_web_trf + hi_core_news_md
- **Libraries:**
    - ffmpeg, RNNoise
    - fasttext/langdetect
    - sentence-transformers
    - pydub, webrtcvad
    - FastAPI for API layer
    - Celery for task queue
    - boto3/e2e SDK for storage
- **Hardware:**
    - 1√ó GPU instance (‚â•16 GB VRAM for Whisper large)
    - 1√ó CPU instance for preprocessing & API
- **Storage:**
    - Object storage bucket per project.
    - Metadata DB: PostgreSQL.









    # CORRECTED Implementation Plan for immediate next steps - Architecture Context

## CRITICAL ARCHITECTURE CLARIFICATIONS

**Current Working System Architecture:**
- **Whisper & Qwen Models**: Running as REMOTE VLLM endpoints on E2E Networks (NOT local)
- **Model Allocation**: 500GB each for Whisper and Qwen on E2E infrastructure
- **Local Server**: 560GB storage for the application itself (FastAPI + Streamlit + LangGraph)
- **Current Flow**: Audio ‚Üí E2E Whisper endpoint ‚Üí E2E Qwen endpoint ‚Üí Results
- **DO NOT**: Attempt local model loading, model downloads, or GPU memory management
- **DO**: Make HTTP requests to existing E2E model endpoints

## MAINTAIN CURRENT WORKING PARTS
- **FastAPI Backend (Port 8000)**: Already working with E2E endpoints - DO NOT MODIFY
- **Streamlit Frontend (Port 8501)**: Current UI working - ONLY ADD NEW FEATURES
- **LangGraph Workflow**: Current transcribe‚Üísummarize flow working - EXTEND ONLY
- **E2E Integration**: HTTP calls to remote endpoints working - PRESERVE EXACTLY

---

# M1: Diarization End-to-End (1-2 days)

## Implementation Strategy

### LangGraph Node Addition
```python
# Add diarize_agent node AFTER transcribe_agent
# Modify WorkflowState to include:
segments: Optional[List[Dict]] = None  # {speaker, start, end, text}
diarization_enabled: Optional[bool] = False
```

### Diarization Processing Logic
**IMPORTANT**: Use Pyannote locally for speaker detection ONLY (lightweight processing)
- **Audio Processing**: Process full audio file locally with Pyannote for speaker timestamps
- **Transcription**: Continue using E2E Whisper endpoint for text (unchanged)
- **Alignment**: Map E2E transcription results to local Pyannote speaker segments
- **Fallback**: If Pyannote fails, gracefully continue with current E2E-only flow

### Memory Considerations for 560GB Server
- Pyannote model (~500MB) loads locally for diarization
- Audio files temporarily stored for processing
- Clean up processed audio files after diarization
- No model downloads or GPU allocation needed

### Streamlit UI Updates
- Add toggle: "Enable Speaker Detection"
- Speaker-tabbed transcript display
- Speaker timeline visualization

---

# M2: Language Detection + Prompt Conditioning (0.5-1 day)

## Implementation Strategy

### Language Detection Processing
- Use lightweight `langdetect` library (no model downloads needed)
- Process transcription segments from E2E Whisper results
- Tag segments with language (en/hi/mixed)

### Prompt Engineering for E2E Qwen
- Modify prompts sent to E2E Qwen endpoint based on detected languages
- Hindi segments: Send Hindi-conditioned prompts to existing Qwen endpoint
- Mixed segments: Send code-switching aware prompts
- **NO CHANGES** to Qwen model itself - only prompt modifications

### Streamlit UI Updates
- Language distribution display
- Language-based filtering
- Language-aware summary presentation

---

# M3: Exports (JSON/MD/SRT) + Endpoints (1 day)

## Implementation Strategy

### Export Generation (Local Processing)
- Generate exports locally on 560GB server
- Create `./exports/{job_id}/` directory structure
- **File Types**: JSON (structured), Markdown (readable), SRT (subtitles)
- **Storage Management**: Implement cleanup for old exports to manage 560GB space

### API Endpoints
```python
# Add to existing FastAPI backend
GET /results/{id}?format=json|md|srt
# File download endpoints with proper headers
# Automated cleanup of old export files
```

### Streamlit UI Updates
- Download buttons for each format
- File preview capabilities
- Export generation progress indicators

---

# UPDATED WorkflowState Structure

```python
class WorkflowState(BaseModel):
    # EXISTING FIELDS - DO NOT MODIFY
    job_id: str
    action: str
    file_path: Optional[str] = None
    text: Optional[str] = None
    transcript: Optional[str] = None
    summary: Optional[Dict[str, Any]] = None
    
    # NEW FIELDS FOR M1-M3
    segments: Optional[List[Dict]] = None  # M1: Speaker segments
    diarization_enabled: Optional[bool] = False  # M1: Feature flag
    language_tags: Optional[List[str]] = None  # M2: Language per segment
    export_paths: Optional[Dict[str, str]] = None  # M3: Generated files
```

---

# CRITICAL IMPLEMENTATION NOTES FOR CURSOR

## What NOT to Change
1. **E2E Model Endpoints**: Do not modify HTTP requests to Whisper/Qwen
2. **FastAPI Core**: Do not change existing job management or upload handling
3. **Current Workflow**: Do not break existing transcribe‚Üísummarize flow
4. **Model Loading**: Do not attempt to load Whisper or Qwen locally

## What TO Add
1. **Pyannote Integration**: Local speaker diarization processing only
2. **Language Detection**: Lightweight local processing with langdetect
3. **Export Generation**: Local file generation and management
4. **UI Extensions**: New features without breaking existing interface

## Storage Management (560GB Server)
- Monitor disk usage with cleanup routines
- Temporary audio files cleanup after processing
- Export file rotation (keep recent, delete old)
- No model storage needed (models are remote)

## Testing Strategy
- Test each milestone with existing E2E endpoints
- Verify fallback mechanisms work
- Ensure current functionality remains intact
- Test with various audio file sizes

## Risk Mitigation
- **Pyannote Issues**: Graceful fallback to current transcription-only flow
- **Storage Issues**: Implement aggressive cleanup and monitoring
- **E2E Endpoint Issues**: Preserve existing error handling
- **Performance**: Process diarization and language detection in parallel where possible

---

# SEQUENTIAL IMPLEMENTATION MANDATORY

## IMPLEMENTATION ORDER: STRICTLY ONE BY ONE

**DO NOT START M2 UNTIL M1 IS 100% COMPLETE AND TESTED**
**DO NOT START M3 UNTIL M2 IS 100% COMPLETE AND TESTED**

### M1 COMPLETION CRITERIA (Must Pass ALL Before M2)
- [ ] Diarization works on 5+ minute audio files
- [ ] Speaker labels consistent and accurate 
- [ ] UI shows speaker-separated transcript clearly
- [ ] Fallback works when diarization disabled
- [ ] **TEST SUITE**: Create and run tests for diarization pipeline
- [ ] **INTEGRATION TEST**: Test with existing E2E endpoints unchanged
- [ ] **USER ACCEPTANCE**: Manual testing confirms no regression in current features

### M2 COMPLETION CRITERIA (Must Pass ALL Before M3)
- [ ] Language detection accuracy >90% on test samples
- [ ] Summaries improved with language-aware prompts
- [ ] UI shows language distribution clearly
- [ ] Code-switching handled gracefully
- [ ] **TEST SUITE**: Create and run tests for language detection
- [ ] **INTEGRATION TEST**: Test M1+M2 together work perfectly
- [ ] **USER ACCEPTANCE**: Manual testing of complete M1+M2 flow

### M3 COMPLETION CRITERIA (Final Milestone)
- [ ] All export formats generate correctly
- [ ] SRT files play correctly in video players
- [ ] Download endpoints work reliably
- [ ] File cleanup prevents disk space issues
- [ ] **TEST SUITE**: Create and run tests for all export formats
- [ ] **INTEGRATION TEST**: Complete M1+M2+M3 pipeline test
- [ ] **USER ACCEPTANCE**: Full system test with real audio files

## TESTING REQUIREMENT FOR EACH MILESTONE
Each milestone must include:
1. **Unit Tests**: Test individual functions/agents
2. **Integration Tests**: Test with existing system
3. **Manual Tests**: Real audio file testing
4. **Regression Tests**: Ensure previous features still work

**CURSOR: DO NOT PROCEED TO NEXT MILESTONE WITHOUT EXPLICIT CONFIRMATION ALL TESTS PASS**